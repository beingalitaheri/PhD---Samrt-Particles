behaviors:
  JetAgent:
    trainer_type: ppo  # Using the Proximal Policy Optimization algorithm
    hyperparameters:
      batch_size: 1024           # Number of experiences sampled from the buffer for each training step
      buffer_size: 10240         # Number of experiences stored in the buffer
      learning_rate: 3.0e-4      # Learning rate for the optimizer
      beta: 5.0e-3               # Entropy regularization factor to encourage exploration
      epsilon: 0.2               # Clipping parameter for PPO to limit policy updates
      lambd: 0.95                # GAE (Generalized Advantage Estimation) lambda for bias-variance trade-off
      num_epoch: 3               # Number of epochs to perform on each batch of experiences
    network_settings:
      normalize: false           # Whether to normalize observations (set to true if observations vary widely)
      hidden_units: 256          # Number of neurons in each hidden layer of the neural network
      num_layers: 2              # Number of hidden layers in the neural network
    reward_signals:
      extrinsic:
        gamma: 0.99              # Discount factor for future rewards
        strength: 1.0             # Strength of the extrinsic reward signal
    max_steps: 3500000             # Maximum number of steps per training run before stopping
    time_horizon: 64               # Number of steps to consider for computing returns
    summary_freq: 10000            # Frequency (in steps) at which to record summaries
    keep_checkpoints: 5            # Number of checkpoints to keep during training
    checkpoint_interval: 50000     # Frequency (in steps) to save checkpoints
    threaded: true                  # Whether to use multi-threading for training
    
